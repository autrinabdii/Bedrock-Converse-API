{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88a5ab2f-d044-4956-b75b-7408d9c3e323",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation with Amazon Bedrock - Retrieving Data Automatically from APIs\n",
    "\n",
    "> *PLEASE NOTE: This notebook should work well with the **`Data Science 3.0`** kernel in SageMaker Studio*\n",
    "\n",
    "---\n",
    "\n",
    "Another important type of retrieval which customers can take advantage of with LLM is **structured data retrieval** from APIs. Structured data retrieval is extremely useful for augmenting LLM applications with up to date information which can be retrieved in a repeatable manner, but the outputs are always changing. An example of a question you might ask an LLM which uses this type of retrieval might be \"How long will it take for my Amazon.com order containing socks to arrive?\". In this notebook, we will show how to integrate an LLM with a backend API service which has the ability to answer a user's question through RAG.\n",
    "\n",
    "Specifically, we will be building a tool which is able to tell you the weather based on natural language. This is a fairly trivial example, but it does a good job of showing how multiple API tools can be used by an LLM to retrieve dynamic data to augment a prompt. Here is a visual of the architecture we will be building today.\n",
    "\n",
    "![api](./images/api.png)\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7cced6",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup `boto3` Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1b35ae8-909f-4306-9a6c-fb7291206b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydantic==1.10.14\n",
      "  Using cached pydantic-1.10.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (150 kB)\n",
      "Collecting typing-extensions>=4.2.0 (from pydantic==1.10.14)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Using cached pydantic-1.10.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: typing-extensions, pydantic\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.12.2\n",
      "    Uninstalling typing_extensions-4.12.2:\n",
      "      Successfully uninstalled typing_extensions-4.12.2\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.14\n",
      "    Uninstalling pydantic-1.10.14:\n",
      "      Successfully uninstalled pydantic-1.10.14\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dash 2.17.1 requires dash-core-components==2.0.0, which is not installed.\n",
      "dash 2.17.1 requires dash-html-components==2.0.0, which is not installed.\n",
      "dash 2.17.1 requires dash-table==5.0.0, which is not installed.\n",
      "langchain-community 0.2.11 requires langchain<0.3.0,>=0.2.12, but you have langchain 0.2.5 which is incompatible.\n",
      "sagemaker-jupyterlab-extension-common 0.1.19 requires pydantic<3,>=1.10.17, but you have pydantic 1.10.14 which is incompatible.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.1.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pydantic-1.10.14 typing-extensions-4.12.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pydantic==1.10.14 --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85063bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from IPython.display import Markdown, display, Pretty\n",
    "\n",
    "region = os.environ.get(\"AWS_REGION\")\n",
    "boto3_bedrock = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name=region,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909f3e58",
   "metadata": {},
   "source": [
    "---\n",
    "## Defining the API Tools\n",
    "\n",
    "The first thing we need to do for our LLM is define the tools it has access to. In this case we will be defining local Python functions, but it important to not that these could be any type of application service. Examples of what these tools might be on AWS include...\n",
    "\n",
    "* An AWS Lambda function\n",
    "* An Amazon RDS database connection\n",
    "* An Amazon DynamnoDB table\n",
    "  \n",
    "More generic examples include...\n",
    "\n",
    "* REST APIs\n",
    "* Data warehouses, data lakes, and databases\n",
    "* Computation engines\n",
    "\n",
    "In this case, we define two tools which reach external APIs below with two python functions\n",
    "1. the ability to retrieve the latitude and longitude of a place given a natural language input\n",
    "2. the ability to retrieve the weather given an input latitude and longitude\n",
    "\n",
    "#### ^^^ The API for getting the latitude is an open source and so we might encounter a throtlling by then ^^^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a897463-0c6a-4cf8-80f9-195b40adf25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install geopy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "427526eb-94b6-42d4-8ec6-ce001ec2ab19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "def get_weather(latitude: str, longitude: str) -> dict:\n",
    "    \"\"\"\n",
    "    Checking the weather using the open-meteo API\n",
    "    \"\"\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36'}\n",
    "    url = f\"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current_weather=true\"\n",
    "    response = requests.get(url)\n",
    "    return response.json()\n",
    "\n",
    "def get_lat_long(place: str) -> dict:\n",
    "    \"\"\"\n",
    "    Geocoding using the Nominatim API\n",
    "    \"\"\"\n",
    "    app = Nominatim(user_agent=\"tutorial\")\n",
    "    location = app.geocode(place).raw\n",
    "\n",
    "    return {\"latitude\": location['lat'], \"longitude\": location['lon']}\n",
    "\n",
    "def call_function(tool_name, parameters):\n",
    "    func = globals()[tool_name]\n",
    "    output = func(**parameters)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b2d1c0",
   "metadata": {},
   "source": [
    "We also define a function called `call_function` which is used to abstract the tool name. You can see an example of determining the weather in Las Vegas below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92319d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather in Las Vegas is...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'latitude': 36.16438,\n",
       " 'longitude': -115.14392,\n",
       " 'generationtime_ms': 0.07796287536621094,\n",
       " 'utc_offset_seconds': 0,\n",
       " 'timezone': 'GMT',\n",
       " 'timezone_abbreviation': 'GMT',\n",
       " 'elevation': 620.0,\n",
       " 'current_weather_units': {'time': 'iso8601',\n",
       "  'interval': 'seconds',\n",
       "  'temperature': '°C',\n",
       "  'windspeed': 'km/h',\n",
       "  'winddirection': '°',\n",
       "  'is_day': '',\n",
       "  'weathercode': 'wmo code'},\n",
       " 'current_weather': {'time': '2024-09-15T16:15',\n",
       "  'interval': 900,\n",
       "  'temperature': 30.6,\n",
       "  'windspeed': 15.9,\n",
       "  'winddirection': 175,\n",
       "  'is_day': 1,\n",
       "  'weathercode': 0}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "place = 'Las Vegas'\n",
    "lat_long_response = call_function('get_lat_long', {'place' : place})\n",
    "weather_response = call_function('get_weather', lat_long_response)\n",
    "print(f'Weather in {place} is...')\n",
    "weather_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18203223",
   "metadata": {},
   "source": [
    "As you might expect, we have to describe our tools to our LLM, so it knows how to use them. The strings below describe the python functions for lat/long and weather to Claude in an XML friendly format which we have seen previously in the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3cb27ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tool_description>\n",
      "<tool_name>get_weather</tool_name>\n",
      "<parameters>\n",
      "<name>latitude</name>\n",
      "<name>longitude</name>\n",
      "</parameters>\n",
      "</tool_description>\n",
      "\n",
      "<tool_description>\n",
      "<tool_name>get_lat_long</tool_name>\n",
      "<parameters>\n",
      "<name>place</name>  \n",
      "</parameters>\n",
      "</tool_description>\n"
     ]
    }
   ],
   "source": [
    "get_weather_description = \"\"\"\\\n",
    "<tool_description>\n",
    "<tool_name>get_weather</tool_name>\n",
    "<parameters>\n",
    "<name>latitude</name>\n",
    "<name>longitude</name>\n",
    "</parameters>\n",
    "</tool_description>\n",
    "\"\"\"\n",
    "\n",
    "get_lat_long_description = \"\"\"\n",
    "<tool_description>\n",
    "<tool_name>get_lat_long</tool_name>\n",
    "<parameters>\n",
    "<name>place</name>  \n",
    "</parameters>\n",
    "</tool_description>\"\"\"\n",
    "\n",
    "list_of_tools_specs = [get_weather_description, get_lat_long_description]\n",
    "tools_string = ''.join(list_of_tools_specs)\n",
    "print(tools_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fac8ab",
   "metadata": {},
   "source": [
    "---\n",
    "## Define Prompts to Orchestrate our LLM Using Tools\n",
    "\n",
    "Now that the tools are defined both programmatically and as a string, we can start orchestrating the flow which will answer user questions. The first step to this is creating a prompt which defines the rules of operation for Claude. In the prompt below, we provide explicit direction on how Claude should use tools to answer these questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6edc9104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "TOOL_TEMPLATE = \"\"\"\\\n",
    "Your job is to formulate a solution to a given <user-request> based on the instructions and tools below.\n",
    "\n",
    "Use these Instructions: \n",
    "1. In this environment you have access to a set of tools and functions you can use to answer the question.\n",
    "2. You can call the functions by using the <function_calls> format below.\n",
    "3. Only invoke one function at a time and wait for the results before invoking another function.\n",
    "4. The Results of the function will be in xml tag <function_results>. Never make these up. The values will be provided for you.\n",
    "5. Only use the information in the <function_results> to answer the question.\n",
    "6. Once you truly know the answer to the question, place the answer in <answer></answer> tags. Make sure to answer in a full sentence which is friendly.\n",
    "7. Never ask any questions\n",
    "\n",
    "<function_calls>\n",
    "<invoke>\n",
    "<tool_name>$TOOL_NAME</tool_name>\n",
    "<parameters>\n",
    "<$PARAMETER_NAME>$PARAMETER_VALUE</$PARAMETER_NAME>\n",
    "...\n",
    "</parameters>\n",
    "</invoke>\n",
    "</function_calls>\n",
    "\n",
    "Here are the tools available:\n",
    "<tools>\n",
    "{tools_string}\n",
    "</tools>\n",
    "\n",
    "<user-request>\n",
    "{user_input}\n",
    "</user-request>\n",
    "\n",
    "Human: What is the first step in order to solve this problem?\n",
    "\n",
    "Assistant:\n",
    "\"\"\"\n",
    "TOOL_PROMPT = PromptTemplate.from_template(TOOL_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bdbf85",
   "metadata": {},
   "source": [
    "---\n",
    "## Executing the Workflow\n",
    "\n",
    "Armed with our prompt and structured tools, we can now write an orchestration function which will iteratively step through the logical tasks to answer a user question. In the cell below we use the `invoke_model` function to generate a response with Claude and the `single_retriever_step` function to iteratively call tools when the LLM tells us we need to. The general flow works like this...\n",
    "\n",
    "1. The user enters an input to the application\n",
    "2. The user input is merged with the original prompt and sent to the LLM to determine the next step\n",
    "3. If the LLM knows the answer, it will answer and we are done. If not, go to next step 4.\n",
    "4. The LLM will determine which tool to use to answer the question.\n",
    "5. We will use the tool as directed by the LLM and retrieve the results.\n",
    "6. We provide the results back into the original prompt as more context.\n",
    "7. We ask the LLM the next step or if knows the answer.\n",
    "8. Return to step 3.\n",
    "\n",
    "If this is a bit confusing do not panic, we will walk through this flow in an example shortly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae086dcf-270d-4981-a5a6-d61a7e4665b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xmltodict in /opt/conda/lib/python3.10/site-packages (0.13.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install xmltodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "59df5a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xmltodict\n",
    "import json\n",
    "\n",
    "def invoke_model(prompt):\n",
    "    client = boto3.client(service_name='bedrock-runtime', region_name=os.environ.get(\"AWS_REGION\"),)\n",
    "    body = json.dumps({\"prompt\": prompt, \"max_tokens_to_sample\": 500, \"temperature\": 0,})\n",
    "    modelId = \"anthropic.claude-instant-v1\"\n",
    "    response = client.invoke_model(\n",
    "        body=body, modelId=modelId, accept=\"application/json\", contentType=\"application/json\"\n",
    "    )\n",
    "    return json.loads(response.get(\"body\").read()).get(\"completion\")\n",
    "\n",
    "def single_retriever_step(prompt, output):\n",
    "    # first check if the model has answered the question\n",
    "    done = False\n",
    "    if '<answer>' in output:\n",
    "        answer = output.split('<answer>')[1]\n",
    "        answer = answer.split('</answer>')[0]\n",
    "        done = True\n",
    "        return done, answer\n",
    "    \n",
    "    # if the model has not answered the question, go execute a function\n",
    "    else:\n",
    "\n",
    "        # parse the output for any \n",
    "        function_xml = output.split('<function_calls>')[1]\n",
    "        function_xml = function_xml.split('</function_calls>')[0]\n",
    "        function_dict = xmltodict.parse(function_xml)\n",
    "        func_name = function_dict['invoke']['tool_name']\n",
    "        parameters = function_dict['invoke']['parameters']\n",
    "\n",
    "        # call the function which was parsed\n",
    "        func_response = call_function(func_name, parameters)\n",
    "\n",
    "        # create the next human input\n",
    "        func_response_str = '\\n\\nHuman: Here is the result from your function call\\n\\n'\n",
    "        func_response_str = func_response_str + f'<function_results>\\n{func_response}\\n</function_results>'\n",
    "        func_response_str = func_response_str + '\\n\\nIf you know the answer, say it. If not, what is the next step?\\n\\nAssistant:'\n",
    "\n",
    "        # augment the prompt\n",
    "        prompt = prompt + output + func_response_str\n",
    "    return done, prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d569ad",
   "metadata": {},
   "source": [
    "Let's start our first example `What is the weather in Las Vegas?`. The code below asks the LLM what the first step is and you will notice that the LLM is able to ascertain it needs to use the `get_lat_long` tool first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ce14d5de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The first step is to invoke the get_lat_long function to get the latitude and longitude of Las Vegas:\n",
       "\n",
       "<function_calls>\n",
       "<invoke>\n",
       "<tool_name>get_lat_long</tool_name>  \n",
       "<parameters>\n",
       "<place>Las Vegas</place>\n",
       "</parameters>\n",
       "</invoke>\n",
       "</function_calls>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "user_input = 'What is the weather in Las Vegas?'\n",
    "\n",
    "next_step = TOOL_PROMPT.format(tools_string=tools_string, user_input=user_input)\n",
    "\n",
    "output = invoke_model(next_step).strip()\n",
    "done, next_step = single_retriever_step(next_step, output)\n",
    "if not done:\n",
    "    display(Pretty(f'{output}'))\n",
    "else:\n",
    "    display(Pretty('Final answer from LLM:\\n'+f'{next_step}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f27a09",
   "metadata": {},
   "source": [
    "Great, Claude has figured out that we should first call the lat and long tool. The next step is then orchestrated just like the first. This time, Claude uses the lat/long from the first request to now ask for the weather of that specific location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "55f62b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The next step is to invoke the get_weather function using the latitude and longitude returned from the previous function call to get the weather in Las Vegas:\n",
       "\n",
       "<function_calls>\n",
       "<invoke>  \n",
       "<tool_name>get_weather</tool_name>\n",
       "<parameters>\n",
       "<latitude>36.1672559</latitude>\n",
       "<longitude>-115.148516</longitude>  \n",
       "</parameters>\n",
       "</invoke>\n",
       "</function_calls>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = invoke_model(next_step).strip()\n",
    "done, next_step = single_retriever_step(next_step, output)\n",
    "if not done:\n",
    "    display(Pretty(f'{output}'))\n",
    "else:\n",
    "    display(Pretty('Final answer from LLM:\\n'+f'{next_step}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eb8dcd",
   "metadata": {},
   "source": [
    "Finally the LLM is able to answer the question based on the input function above. Very cool!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56361993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Final answer from LLM:\n",
       "\n",
       "The weather in Las Vegas is 30.6 degrees Celsius with 15.9 km/h winds from the southeast.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = invoke_model(next_step).strip()\n",
    "done, next_step = single_retriever_step(next_step, output)\n",
    "if not done:\n",
    "    display(Pretty(f'{output}'))\n",
    "else:\n",
    "    display(Pretty('Final answer from LLM:\\n'+f'{next_step}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d64a45a",
   "metadata": {},
   "source": [
    "Let's try another example to show how a different place (Singapore) can be used in this example. Notice how we set the for loop to 5 iterations even though the model only uses 3 of these. This iteration capping is common in agent workflows and should be tuned according to your use case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d1ff52cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The first step is to invoke the get_lat_long function to get the latitude and longitude of Singapore, since that information is needed by the get_weather function to return the weather.\n",
       "\n",
       "<function_calls>\n",
       "<invoke>\n",
       "<tool_name>get_lat_long</tool_name>  \n",
       "<parameters>\n",
       "<place>Singapore</place>\n",
       "</parameters>\n",
       "</invoke>\n",
       "</function_calls>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "The next step is to invoke the get_weather function, passing in the latitude and longitude retrieved from the previous get_lat_long call, to get the weather in Singapore.\n",
       "\n",
       "<function_calls>\n",
       "<invoke>  \n",
       "<tool_name>get_weather</tool_name>\n",
       "<parameters>\n",
       "<latitude>1.357107</latitude>\n",
       "<longitude>103.8194992</longitude>  \n",
       "</parameters>\n",
       "</invoke>\n",
       "</function_calls>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Final answer from LLM:\n",
       "\n",
       "The weather in Singapore is 27.0 degrees Celsius, with a wind speed of 5.1 km/h from the southeast (184 degrees) and overcast skies (weather code 3).\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "user_input = 'What is the weather in Singapore?'\n",
    "next_step = TOOL_PROMPT.format(tools_string=tools_string, user_input=user_input)\n",
    "\n",
    "for i in range(5):\n",
    "    output = invoke_model(next_step).strip()\n",
    "    done, next_step = single_retriever_step(next_step, output)\n",
    "    if not done:\n",
    "        display(Pretty(f'{output}'))\n",
    "    else:\n",
    "        display(Pretty('Final answer from LLM:\\n'+f'{next_step}'))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "532481a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The first step is to invoke the get_lat_long function to get the latitude and longitude of Chandigarh, India since that information is needed by the get_weather function.\n",
       "\n",
       "<function_calls>\n",
       "<invoke>\n",
       "<tool_name>get_lat_long</tool_name>  \n",
       "<parameters>\n",
       "<place>Chandigarh, India</place>\n",
       "</parameters>\n",
       "</invoke>\n",
       "</function_calls>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "The next step is to invoke the get_weather function using the latitude and longitude retrieved from the previous function call to get the weather in Chandigarh, India.\n",
       "\n",
       "<function_calls>\n",
       "<invoke>  \n",
       "<tool_name>get_weather</tool_name>\n",
       "<parameters>\n",
       "<latitude>30.7334421</latitude> \n",
       "<longitude>76.7797143</longitude>\n",
       "</parameters>\n",
       "</invoke>\n",
       "</function_calls>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Final answer from LLM:\n",
       "\n",
       "The weather in Chandigarh, India is 26.5 degrees Celsius, with a wind speed of 1.8 km/h blowing from the east.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "user_input = 'What is the weather in Chandigarh, India?'\n",
    "next_step = TOOL_PROMPT.format(tools_string=tools_string, user_input=user_input)\n",
    "\n",
    "for i in range(5):\n",
    "    output = invoke_model(next_step).strip()\n",
    "    done, next_step = single_retriever_step(next_step, output)\n",
    "    if not done:\n",
    "        display(Pretty(f'{output}'))\n",
    "    else:\n",
    "        display(Pretty('Final answer from LLM:\\n'+f'{next_step}'))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf5d5e9",
   "metadata": {},
   "source": [
    "---\n",
    "## Leveraging Open Source Agents with Bedrock\n",
    "\n",
    "As with the previous notebook, there are many ways to orchestrate agent workflows with LLMs. One of the popular open source frameworks for this is [LangChain's agent module](https://python.langchain.com/docs/modules/agents/). LangChain provides a variety of agent types as well as the same [integration to Amazon Bedrock](https://python.langchain.com/docs/integrations/llms/bedrock) being available for these agent based workflows. Let's work on implementing the same workflow as above into the LangChain ecosystem.\n",
    "\n",
    "First, we can define our LLM with the `Bedrock` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "722f5f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "agent_llm = Bedrock(\n",
    "    client=boto3.client(service_name='bedrock-runtime', region_name=os.environ.get(\"AWS_REGION\"),),\n",
    "    model_id=\"anthropic.claude-instant-v1\",\n",
    "    model_kwargs={\n",
    "        \"max_tokens_to_sample\": 500,\n",
    "        \"temperature\": 0.0,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "406909ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "import ast\n",
    "\n",
    "@tool (\"get_lat_long\")\n",
    "def get_lat_long(place: str) -> dict:\n",
    "    \"\"\"\n",
    "    Geocoding using the Nominatim API\n",
    "    \"\"\"\n",
    "    app = Nominatim(user_agent=\"tutorial\")\n",
    "    location = app.geocode(place).raw\n",
    "\n",
    "    return {\"latitude\": location['lat'], \"longitude\": location['lon']}\n",
    "\n",
    "@tool (\"get_weather_dict\")\n",
    "def get_weather_dict(co_ord_str: str) -> dict:\n",
    "    \"\"\"\n",
    "    Returns weather data for a given a dictionary having latitude and longitude.\n",
    "    \"\"\"\n",
    "    if '{' in co_ord_str:\n",
    "        co_ord_str = ast.literal_eval(co_ord_str)\n",
    "        latitude = co_ord_str['latitude']\n",
    "        longitude = co_ord_str['longitude']\n",
    "    else:\n",
    "        latitude = float(co_ord_str.split(\",\")[0].strip())\n",
    "        longitude = float(co_ord_str.split(\",\")[1].strip())\n",
    "    url = f\"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current_weather=true\"\n",
    "    response = requests.get(url)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7e139d",
   "metadata": {},
   "source": [
    "As well as being able to define our own tools, LangChain has some pre-built tools which are general purpose. One well documented issue with LLMs is that they are not good at arithmetic only from natural language. We can use the `LLMMathChain` from LangChain to help solve this problem. In this chain, we are able to ask an LLM simple math questions and the question will be sent to a calculator in order to return a perfect mathematical result. An example is included below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9482e895-f770-44f7-8d6c-71b2feffb102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numexpr in /opt/conda/lib/python3.10/site-packages (2.10.1)\n",
      "Requirement already satisfied: numpy>=1.23.0 in /opt/conda/lib/python3.10/site-packages (from numexpr) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install numexpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "10784e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
      "What is nine times 465?\u001b[32;1m\u001b[1;3m ```text\n",
      "9 * 465\n",
      "```\u001b[0m\n",
      "Answer: \u001b[33;1m\u001b[1;3m4185\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain import LLMMathChain\n",
    "\n",
    "math_tool_template = \"\"\"\\\n",
    "Given a question from the Human with a math problem, provide only a single line mathematical expression that solves the problem in the following format.\n",
    "Never solve the expression only create a parsable expression.\n",
    "```text\n",
    "${{single line mathematical expression that solves the problem}}\n",
    "```\n",
    "\n",
    "Here is an example response with a single line mathematical expression for solving a math problem:\n",
    "```text\n",
    "37593**(1/5)\n",
    "```\n",
    "\n",
    "Human: {question}\n",
    "\n",
    "Assistant:\"\"\"\n",
    "\n",
    "llm_math_chain = LLMMathChain.from_llm(llm=agent_llm, verbose=True)\n",
    "llm_math_chain.llm_chain.prompt.template = math_tool_template\n",
    "result = llm_math_chain('What is nine times 465?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8379ea",
   "metadata": {},
   "source": [
    "Now that all the tools are defined, lets create a [ReACT agent](https://python.langchain.com/docs/modules/agents/agent_types/react) (Reason + Act) which will leverage the LLM to generate the next set of actions to solve a given question. This next cell creates the agent from a list of tools we provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bf5c67dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "tools_list = [get_lat_long, get_weather_dict]\n",
    "tools_list.append(\n",
    "    Tool.from_function(\n",
    "        func=llm_math_chain.run,\n",
    "        name=\"Calculator\",\n",
    "        description=\"Useful for when you need to answer questions about math.\",\n",
    "    )\n",
    ")\n",
    "react_agent = initialize_agent(\n",
    "    tools_list, \n",
    "    agent_llm, \n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
    "    verbose=True,\n",
    "    max_iteration=4,\n",
    "    return_intermediate_steps=True,\n",
    "    handle_parsing_errors=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82281d22",
   "metadata": {},
   "source": [
    "The ReAct framework also expects a specific format to be followed by our LLM. This custom prompt adds place holders for the 3 components which need to be followed.\n",
    "\n",
    "1. **Action** which is the tool to use\n",
    "2. **Action Input** is the parameters coming from the LLM which will go into the tool or function to be called\n",
    "3. **Observation** is the final result of the call from the tool\n",
    "\n",
    "These three components to ReAct are recursively executed until there is an answer to the question or we reach the limit of execution steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f5755c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Use the following format:\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do, Also try to follow steps mentioned above\n",
    "Action: the action to take, should be one of [ \"get_lat_long\", \"get_weather_dict\", \"Calculator\"]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Question: {input}\n",
    "\n",
    "Assistant:\n",
    "{agent_scratchpad}\"\"\"\n",
    "\n",
    "react_agent.agent.llm_chain.prompt.template=prompt_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466544d1",
   "metadata": {},
   "source": [
    "Now we can send in the first question just like the one we used in the DIY system. Notice how the logging allows you to see how the LLM is reasoning through the task within the ReAct framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "261d3cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m Here is the response in the specified format:\n",
      "\n",
      "Question: Can you check the weather in Las Vegas for me?\n",
      "Thought: I need to get the latitude and longitude of Las Vegas first\n",
      "Action: get_lat_long \n",
      "Action Input: Las Vegas\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m{'latitude': '36.1672559', 'longitude': '-115.148516'}\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Now I can use the latitude and longitude to get the weather details\n",
      "Action: get_weather_dict\n",
      "Action Input: 36.1672559, -115.148516\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m{'latitude': 36.16438, 'longitude': -115.14392, 'generationtime_ms': 0.07402896881103516, 'utc_offset_seconds': 0, 'timezone': 'GMT', 'timezone_abbreviation': 'GMT', 'elevation': 620.0, 'current_weather_units': {'time': 'iso8601', 'interval': 'seconds', 'temperature': '°C', 'windspeed': 'km/h', 'winddirection': '°', 'is_day': '', 'weathercode': 'wmo code'}, 'current_weather': {'time': '2024-09-15T16:15', 'interval': 900, 'temperature': 30.6, 'windspeed': 15.9, 'winddirection': 175, 'is_day': 1, 'weathercode': 0}}\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now have the weather details for Las Vegas\n",
      "Final Answer: The current weather in Las Vegas is 30.6°C, with a wind speed of 15.9 km/h coming from 175°. The sky is clear.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "result = react_agent(\"Can you check the weather in Las Vegas for me?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4051a8",
   "metadata": {},
   "source": [
    "Another example is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "653e8139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m Here is the response in the specified format:\n",
      "\n",
      "Question: can you check the weather in Seattle WA for me?\n",
      "Thought: I need to get the latitude and longitude of Seattle first\n",
      "Action: get_lat_long \n",
      "Action Input: Seattle WA\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m{'latitude': '47.6038321', 'longitude': '-122.330062'}\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Now I can use the latitude and longitude to get the weather details\n",
      "Action: get_weather_dict\n",
      "Action Input: 47.6038321, -122.330062\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m{'latitude': 47.595562, 'longitude': -122.32443, 'generationtime_ms': 0.051975250244140625, 'utc_offset_seconds': 0, 'timezone': 'GMT', 'timezone_abbreviation': 'GMT', 'elevation': 40.0, 'current_weather_units': {'time': 'iso8601', 'interval': 'seconds', 'temperature': '°C', 'windspeed': 'km/h', 'winddirection': '°', 'is_day': '', 'weathercode': 'wmo code'}, 'current_weather': {'time': '2024-09-15T16:15', 'interval': 900, 'temperature': 13.4, 'windspeed': 11.7, 'winddirection': 11, 'is_day': 1, 'weathercode': 1}}\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now have the weather details for Seattle\n",
      "Final Answer: The current weather in Seattle, WA is 13.4°C with 11.7 km/h winds coming from the north northeast.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "result = react_agent(\"can you check the weather in Seattle WA for me?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34784a47",
   "metadata": {},
   "source": [
    "In this last example, we will give a different type of problem to the LLM where it will be able to take advantage of the math tool as well as the lat/long tool instead of only talking about weather."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1aeb65f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m Here is the response in the specified format:\n",
      "\n",
      "Question: what is the latitude of Washington DC multiplied by 2\n",
      "\n",
      "Thought: I need to first get the latitude of Washington DC\n",
      "Action: get_lat_long \n",
      "Action Input: Washington DC\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m{'latitude': '38.8950368', 'longitude': '-77.0365427'}\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Now I need to multiply the latitude by 2\n",
      "Action: Calculator\n",
      "Action Input: 38.8950368 * 2\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
      "38.8950368 * 2\u001b[32;1m\u001b[1;3m ```text\n",
      "38.8950368 * 2\n",
      "```\u001b[0m\n",
      "Answer: \u001b[33;1m\u001b[1;3m77.7900736\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[38;5;200m\u001b[1;3mAnswer: 77.7900736\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: 77.7900736\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "result = react_agent(\"what is the latitude of Washington DC multiplied by 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70b217d",
   "metadata": {},
   "source": [
    "---\n",
    "## Next steps\n",
    "\n",
    "Now that you have used a few different retrieval systems, lets move on to the next notebook where you can apply the skills you've learned so far!"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
